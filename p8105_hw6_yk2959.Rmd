---
title: "p8105_hw6_yk2959"
author: "Kiran Kui"
date: "2022-11-29"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(readxl)
library(dplyr)
library(purrr)
library(ggplot2)
library(modelr)
```

# Problem 1

To obtain a distribution for $\hat{r}^2$, I will draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. I used `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
nynoaa_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
nynoaa_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

The $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
nynoaa_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 


# Problem 2

```{r}
homicide_df = read_csv("./data/homicide-data.csv") %>%
  janitor::clean_names() 

skimr::skim(homicide_df)
```

I have imported the homicide dataset from the github repository and tidied the names of the variables in the dataset. 

## Brief introduction of the raw homicide dataset 

The raw dataset contains `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns, with each row representing a single record of homicide in 50 large U.S. cities. 

Variables include, date of homicide, victim first and last name, victims' race, age, sex and the city/state that the homicide was located, as well as the lattitude and longitude of the homicide location, given in `r colnames(homicide_df)`. 

```{r}
homicide_df_clean_1 = homicide_df %>% 
  unite(city_state, c(city, state), sep = ", ", remove = FALSE) %>% 
  select(-city, -state) %>% 
  mutate(
  resolved = ifelse(
  disposition %in% c('Closed without arrest', 'Open/No arrest'), 0, 1)) %>% 
    filter(!city_state %in% c('Dallas, TX', 'Phoenix, AZ', 'Kansas City, MO', 'Tulsa, AL')) %>%    filter(victim_race %in% c('Black', 'White')) %>%
  mutate(victim_age = as.numeric(victim_age))

homicide_df_clean_1
```

I create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. I then omitted cities Dallas, TX; Phoenix, AZ; and Kansas City, MO because these don’t report victim race and also omit Tulsa, AL because this is a data entry mistake. I limited my analysis those for whom victim_race is white or black.I converted victim_age from a character to a numeric variable. 

```{r}
fit_logistic = homicide_df_clean_1 %>% 
  filter(city_state %in% c('Baltimore, MD')) %>% 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    odds_ratio = exp(estimate),
    lower_CI = exp(estimate - 1.96 * std.error),
    upper_CI = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, odds_ratio, lower_CI, upper_CI) %>%
  filter(term == "victim_sexMale") %>% 
  knitr::kable(digits = 3)

fit_logistic

summary(fit_logistic)

```

For the city of Baltimore, MD, I used the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 
I saved the output of glm as an R object and applied the broom::tidy to this object. 
The estimate and 95% confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims is 0.426 and (0.325, 0.558) respectively. 

```{r}
homicide_df_clean_2 = 
  homicide_df_clean_1 %>% 
  nest(data = -city_state) %>% 
mutate(
  all_countries = 
    map(.x = data, ~glm(glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial()))),
  result = map(all_countries, broom::tidy) 
  ) %>% 
select(city_state, result) %>% 
 unnest(result) %>% 
  mutate(
    odds_ratio = exp(estimate),
    lower_CI = exp(estimate - 1.96 * std.error),
    upper_CI = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, odds_ratio,lower_CI, upper_CI) %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, odds_ratio)) 

ggplot_q2 = 
  homicide_df_clean_2 %>% 
  ggplot(aes(x = city_state, y = odds_ratio)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Plot showing the estimated ORs and CIs for each city",
    x = "City, State",
    y = "Odds Ratio")

ggplot_q2

```

I ran a glm for each of the cities in your dataset, and extracted the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. I did this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

I created a plot that shows the estimated ORs and CIs for each city and organized cities according to estimated OR. 

The adjusted odds ratio for solving homicides comparing male victims to female victims is highest in Albuquerque, NM and lowest in New York, NY. 

# Problem 3

```{r}
birthweight_df = read_csv("./data/birthweight.csv") %>%
  janitor::clean_names() 

skimr::skim(birthweight_df)
```

I have imported the birthweight dataset from the csv file and tidied the names of the variables in the dataset. 

## Brief introduction of the birthweight dataset 

The raw dataset contains `r nrow(birthweight_df)` rows and `r ncol(birthweight_df)` columns, with each row representing a single record of a child. 

Variables include:

* details of the baby such as birthweight in grams, sex of the baby, head circumference, length at birth(cm), etc. 
* details of the mother, such as mother’s weight at delivery (pounds), mother’s age at menarche (years) and mother’s height (inches). 
* one sociodemographic variables about the family was also present - family monthly income (in hundreds, rounded). 

__The full list of variabes is given in `r colnames(birthweight_df)`.__

```{r}
birthweight_df_clean = birthweight_df %>% 
  mutate(malform = as.factor(malform),
         babysex = as.character(babysex),
         mrace = as.character(mrace),         
         frace = as.character(frace)) 

```
I tidied the dataset by double checking if there was missing values (there was none), converting malform into a factor variable, and babysex, mrace and frace into character variables. 

```{r}

my_birthweight_model = lm(bwt ~ babysex + smoken + ppbmi + mrace + gaweeks, data = birthweight_df_clean) 

my_birthweight_model_tidy = lm(bwt ~ babysex + smoken + ppbmi + mrace + gaweeks, data = birthweight_df_clean) %>% 
  broom::tidy() %>% 
  knitr::kable()

my_birthweight_model_tidy

```

I proposed a hypothesized regression model for birthweight based on a [PubMed article](https://pubmed.ncbi.nlm.nih.gov/7114129/#:~:text=The%20sex%20of%20the%20infant,be%20important%20and%20significant%20factors), which describes the important and signficant determinants of birthweight.

The following are determinants of birthweight reported that is also present in our dataset: 

* sex of the baby (babysex)
* average number of cigarettes smoked per day during pregnancy  (smoken)
* maternal BMI pre-pregnancy (ppbmi)
* maternal race (mrace), __differences in baby birthweights between racial groups are due to underlying structural racism__
* gestational age in weeks (gaweeks)

Based on the model, all parameter estimates are significant with a p value of smaller than 0.05. As such, this shows that my model is reasonable and there is no need to remove any insignificant predictors using a backwards stepwise approach. 

```{r}
birthweight_df_clean %>% 
  add_residuals(my_birthweight_model) %>%
    add_predictions(my_birthweight_model) %>% 
      ggplot(aes(x = pred, y = resid)) + geom_point()

```

I plotted a model of residuals against fitted values by using add_predictions and add_residuals in making this plot. The residual plot shows that there is  random scatter of points forming an approximately constant width band around the line where residual = 0, showing that the variance is approximately constant. 

```{r}
main_eff = lm(bwt ~ blength + gaweeks, data = birthweight_df_clean) 

interaction = lm(bwt ~ bhead * blength * babysex, data = birthweight_df_clean) 

```

Compare my model to:

* One using length at birth and gestational age as predictors (main effects only) - *main_eff*
* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these - *interaction*

```{r}
comparison_df = 
  crossv_mc(birthweight_df, 100) %>% 
   mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

comparison_df = 
  comparison_df %>%  
  mutate(my_birthweight_model = map(train, ~lm(bwt ~ blength + babysex + malform, data = .x)),
         main_eff = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         interaction = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_my_birthweight_model = map2_dbl(my_birthweight_model, test, ~rmse(model = .x, data = .y)),
    rmse_main_eff = map2_dbl(main_eff, test, ~rmse(model = .x, data = .y)),
    rmse_interaction = map2_dbl(interaction, test, ~rmse(model = .x, data = .y)))

cvp_error_df = 
comparison_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

cvp_error_df
```

Basedon on the violin plot, the model using head circumference, length, sex, and all interactions (Model named interactions) has the lowest RMSE, which tells us that it’s able to fit the dataset the best out of the three potential models.
